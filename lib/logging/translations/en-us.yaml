loggingPage:
  targetNav:
    experimental: Experimental
    tips:
      caseA: Logging is disabled in the current {pageScope}.
      caseB: The current logging target is <code class="text-capitalize">{currentTarget}</code>, Click the Save button below will disable logging in current {pageScope}.
      caseC: The current logging target is <code class="text-capitalize">{currentTarget}</code>,
      caseD: click the Save button below to set <span class="text-info text-capitalize">{targetType}</span> as the logging target.
      caseE: click the Save button below to update the <span class="text-info text-capitalize">{targetType}</span> configuration.
      caseF: No logging target, click the Save button below to set <code class="text-capitalize">{targetType}</code> as the logging target.
  targetTypes:
    embedded: Embedded
    elasticsearch: Elasticsearch
    splunk: Splunk
    kafka: Kafka
    syslog: syslog
    disable: None
  endpoint: Endpoint
  endpointPlaceholder: 'e.g. https://192.168.1.10:9200'
  logging: Logging
  clusterHeader: Cluster Logging
  projectHeader: Project Logging
  helpText:
    cluster: We will collect the standard output and standard error for each container, the log files which under path <code >/var/log/containers/</code> on each host. The log will be shipped to the target you choose below.
    clusterTarget: The cluster logging target is <code class="text-capitalize">{clusterTargetType}</code>. If project logging is enabled, logs will be shipped to both cluster target and project logging target.
    noClusterTarget: Logging is disabled at current cluster.
  tags:
    keyPlaceholder: e.g. foo
    valuePlaceholder: e.g. bar
    addActionLabel: Add Field
  keyValueForm:
    keyPlaceholder: e.g. 192.168.1.10
  targetKafka:
    addActionLabel: Add Broker
    host: Host
    port: Port
  embedded:
    elasticsearchEndpoint: 'Elascticsearch Endpoint:'
    kibanaEndpoint: 'Kibana Endpoint:'
    header: Embedded Elasticsearch Configuration
    failed: Failed
    pending: Deploying
    helpText:
      persistantStorage: Embedded Elasticsearch is experimental, no persistent storage will be provided.
      requirements: The embedded deployment includes Elasticsearch and Kibana. Elasticsearch requests at least 1 CPU and 500M MEM available on the node it is deployed.
    cpuRequests:
      label: CPU Requests (Core)
      placeholder: e.g. 1
      helpText: At least 1 CPU (Core) is required
    cpuLimits:
      label: CPU Limits (Core)
      placeholder: e.g. 4
      helpText: At least 1 CPU (Core) is required
    memRequests:
      label: Memory Requests (M)
      placeholder: e.g. 512
      helpText: At least 512M RAM is required
    memLimits:
      label: Memory Limits (M)
      placeholder: e.g. 2000
      helpText: At least 512M RAM is required
    resouceRequestsAndLimits: CPU and Memory

    indexPatterns:
      header: Index Patterns
      helpText: Index patterns are used to generate Elacticsearch index
      prefix: Prefix
      prefixPlaceholder: e.g. logstash
      dateFormat: 'Date Format:'
    generatedIndex: 'The generated index will be: <code class="text-italic">{esIndex}</code>, by default the index patters is $\{clusterName\}-$\{dateFormat\}'
  logPreview:
    header: Log Format Preview
  additional:
    header: Additional Logging Configuration
    fields:
      header: Custom Log Fields
      helpText: You can add custom fields as key/value pairs for better filtering.
    flushInterval:
      header: Flush Interval
      label: Flush Interval
      placeholder: e.g. 1
      sec: sec
      helpText: How often buffered logs would be flushed.

  elasticsearch:
    header: Elasticsearch Configuration
    endpointHelpText: Copy your endpoint from Elastic Cloud, or input the reachable endpoint of your self-hosted Elacticsearch.
    endpointProtocolError: Endpoint should start with "http://" or "https://".
    endpointHostError: Please enter host name or domain name.
    xpack:
      header: X-Pack Security
      headerOptional: (optional)
      helpText: If your Elasticsearch use X-Pack built-in native authentication features, put your username and password below.
      username: Username
      usernamePlaceholder: e.g. John
      password: Password
      passwordPlaceholder: Your Password
    indexPatterns:
      header: Index Patterns
      helpText: Index patterns are used to generate Elacticsearch index
      prefix: Prefix
      prefixPlaceholder: e.g. logstash
      dateFormat: 'Date Format:'
    generatedIndex: 'The generated index will be: <code class="text-italic">{esIndex}</code>, by default the index patters is {indexFormat}'
  splunk:
    header: Splunk HTTP Event Collector Configuration
    helpText: 'You can find instructions on how to configre Splunk HEC(HTTP Event Collector) <a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector" target="_blank">here</a>.'
    token: Token
    tokenPlaceholder: Your Token
    tokenHelpText: 'Tokens are entities that let logging agents and HTTP clients connect to the HEC input. <a href="http://docs.splunk.com/Documentation/Splunk/7.0.0/Data/UsetheHTTPEventCollector#Configure_HTTP_Event_Collector_on_Splunk_Enterprise" target="_blank">Detail</a>'
    source: Source
    sourcePlaceholder: e.g. fluentd
    sourceHelpText: 'A default field that identifies the source of an event, that is, where the event originated. <a href="https://docs.splunk.com/Splexicon:Source" target="_blank">Detail</a>'
    index: Index
    indexPlaceholder: e.g. main
    indexHelpText: 'The index you specify here must within the list of this tokenâ€™s allowed indexes. <a href="https://docs.splunk.com/Splexicon:Index" target="_blank">Detail</a>'
  kafka:
    header: Kafka Configuration
    endpointType: Endpoint Type
    zookeeper: Zookeeper
    broker: Broker
    brokerTypeHelpText: Use either Zookeeper or Broker list as the Kafka connection entrypoint.
    zookeeperHelpText: Zookeeper is for building coordination, configuration management, leader detection, detecting node update in kafka cluster.
    brokkerHelpText: A Kafka cluster consists of one or more Brokers, config the host and port for each Broker.
    addEndpoint: Add Endpoint
    topic: Topic
    topicPlaceholder: e.g. message
    topicHelpText: Logs will be send to this topic
  syslog:
    endpointPlaceholder: 'e.g. 192.168.1.10:514'
    header: Syslog Configuration
    endpointHelpText: Input your syslog server endpoint herecluster.
    program: Program
    programPlaceholder: e.g. MyProgram
    programHelpText: The program name of the log.
    severityHelpText: '<p class="text-info text-small">The severity of logs, list of severities definition could be found here <a href="https://tools.ietf.org/html/rfc5424#section-6.2.1" target="_blank">here.</a></p>'
    severities:
      emergency: Emergency
      alert: Alert
      critical: Ctitical
      error: Error
      warning: Warning
      notice: Notice
      info: Info
      debug: Debug
    tokenHelpText: Will add token to structured data in every syslog message. For cloud syslog like <a href="https://help.sumologic.com/Send-Data/Sources/02Sources-for-Hosted-Collectors/Cloud-Syslog-Source" target="_blank">Sumologic</a>, <a href="https://www.loggly.com/docs/customer-token-authentication-token" target="_blank">Loggly</a> etc, you could generate token on their configure page.
  ssl:
    sslHeader: SSL Configuration
    certificate:
      label: Trusted Server Certificate
    clientKey:
      label: Client Private Key
    clientCert:
      label: Client Certificate
    clientKeyPass:
      label: Client Key Password
      password:
        placeholder: Your Client Key Password.
    verify:
      label: SSL Verify
      enabled: Enabled - Input trusted server certificate
      disabled: Disabled
  dockerRootDir:
    header: Docker Configuration
    label: Docker Root Directory
    placeholder: Enter docker root Directory, default value is {dir}
